{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stadnamn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5JC8Dn87_8w"
      },
      "source": [
        "# Generate Norwegian stadnamn using transformer\n",
        "\n",
        "The goal of this weekend project is to play with the transformer architecture and at the same time get to use some Norwegian language data in the way they are manifested in Norwegian place names, stadnamn. The general idea is to teach the transformer to predict the next character in a place name. I thought it could work like a typical autocomplete, so that it can complete a place name once you seed it with zero, one, two or more letters. The transformer code is from the Keras example [text classification with transformer](https://keras.io/examples/nlp/text_classification_with_transformer/) by [Nandan Apoorv](https://twitter.com/NandanApoorv) and I have adapted it somewhat to this task. The transformer architecture was introduced in the [Attention is all you need](https://arxiv.org/abs/1706.03762) paper first submitted to the Arxiv server in 2017. As a side note, it was submitted on my birthday. I really admire the ingenuity of the transformer architecture and me and my collegue Lubos Steskal had a great session dissecting it on the blackboard. I like how this task combines the old Norwegian place names with the fairly new transformer architecture. The neural network should learn quite a bit about how Norwegian place names are composed and it will be fun to see whether it can come up with new ones that have the look and feel of a Norwegian place name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a5wh5PckPUW"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files,drive\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "import json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Duie5zuVlxT6",
        "outputId": "01c19b35-c50c-4d73-f3d2-e870b9798ec9"
      },
      "source": [
        "tf.keras.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u142_xkSkjf2",
        "outputId": "c71705b8-0642-4cfd-d464-86ca69097273"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElGtfMfGkpdz",
        "outputId": "74ef4bd2-e71c-422f-d582-21829ece7b69"
      },
      "source": [
        "os.listdir('/content/drive/MyDrive/stadnamn')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Basisdata_0000_Norge_25833_StedsnavnKomplettSSR_GML.zip',\n",
              " 'stadnamn.csv',\n",
              " 'extract.sh',\n",
              " 'model1.tf']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO6EaCtwlWhv"
      },
      "source": [
        "fn='/content/drive/MyDrive/stadnamn/stadnamn.csv'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DGHreTDnu6D"
      },
      "source": [
        "with open(fn,'r') as fh:\n",
        "  data=fh.read()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjelUk0xN9CN"
      },
      "source": [
        "counter=Counter(data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMFpm0xcFFbE",
        "outputId": "421d57a9-5c60-4d41-dd72-3fdf663fe84c"
      },
      "source": [
        "counter.most_common()[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('e', 1679274),\n",
              " ('\\n', 1265312),\n",
              " ('n', 1196951),\n",
              " ('a', 1102292),\n",
              " ('r', 897382)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jILtjOd3FVNo"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_8gL1OWMboY"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVd80A3gGMxQ"
      },
      "source": [
        "# Find the longest place name and assign the number of characters to maxlen\n",
        "maxlen=0\n",
        "for sn in data.splitlines():\n",
        "  if len(sn) > maxlen:\n",
        "    maxlen=len(sn)\n",
        "    maxlen_sn=sn\n",
        "# Add one to account for the start token, the end token shall not be a feature, only a target\n",
        "maxlen+=1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyNt-FGTG3cb",
        "outputId": "fe211946-6897-4813-812d-e9137d46e657"
      },
      "source": [
        "maxlen"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VbenukRFG48V",
        "outputId": "53cebe63-490d-40ad-b207-d7e45565a485"
      },
      "source": [
        "maxlen_sn"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bergsåsen naturreservat og plantelivsfredningsområde'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCBynfzjwkYF"
      },
      "source": [
        "# Use characters that don't occur in the place names as start and stop characters.\n",
        "start_stop_chars=['@','$']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-tbHNLcql62"
      },
      "source": [
        "# Here we make a dictionary mapping tokens to characters. Later we will zero pad which is why we start at 1\n",
        "tokens=dict([(x,i+1) for i,x in enumerate(sorted(start_stop_chars+list(counter.keys())))])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqUE9TyJCtsY"
      },
      "source": [
        "n_tokens=len(tokens)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO5REb2Dsjvy",
        "outputId": "3f37bc0f-1518-4e40-d580-80d1a9af2df0"
      },
      "source": [
        "data.split('\\n')[:5]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bekkevoll',\n",
              " 'Fjærlandstunnelen',\n",
              " 'Fjærland',\n",
              " 'Lokkaren',\n",
              " 'Skåbudalen naturreservat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN0hD_OW2KQB"
      },
      "source": [
        "## Tokenize place name including start and stop tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDdfEfeuxim2"
      },
      "source": [
        "def tokenize(txt,tokens,start_stop_chars):\n",
        "  sn_tokens=[tokens[start_stop_chars[0]]]\n",
        "  for c in txt:\n",
        "    sn_tokens.append(tokens[c])\n",
        "  sn_tokens.append(tokens[start_stop_chars[1]])\n",
        "  return sn_tokens"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYBk7UPwyqw1",
        "outputId": "5877d72e-cce9-496a-93e8-ed62ce350bc6"
      },
      "source": [
        "# Test with the place where I grew up\n",
        "tokenize('Bulken',tokens,start_stop_chars)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[25, 27, 73, 64, 63, 57, 66, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wet_sLbO2xiU"
      },
      "source": [
        "Split place name into multiple training sequences so that the model can learn the next character from nothing, from only the start character, from the two first characters and so on until we get to the stop character at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIDGNkR_yz6n"
      },
      "source": [
        "def xysetfromstadnamn(txt,tokens,start_stop_chars):\n",
        "  sn_tokens=tokenize(txt,tokens,start_stop_chars)\n",
        "  sn_parts=[]\n",
        "  ytokens=[]\n",
        "  for i in range(1,len(sn_tokens)):\n",
        "    sn_parts.append(sn_tokens[:i])\n",
        "    ytokens.append(sn_tokens[i])\n",
        "  return sn_parts,ytokens"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3BrKL6R4JgC",
        "outputId": "7b199a29-5461-48c9-df75-acc52b73ac7e"
      },
      "source": [
        "# Encode features and targets, the output shows how the training data is organized\n",
        "# The last line represent the target tokens\n",
        "xysetfromstadnamn('Bulken',tokens,start_stop_chars)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[25],\n",
              "  [25, 27],\n",
              "  [25, 27, 73],\n",
              "  [25, 27, 73, 64],\n",
              "  [25, 27, 73, 64, 63],\n",
              "  [25, 27, 73, 64, 63, 57],\n",
              "  [25, 27, 73, 64, 63, 57, 66]],\n",
              " [27, 73, 64, 63, 57, 66, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uncl79H5FqZ"
      },
      "source": [
        "# Define function that zero pads stadnamn and collects corresponding target\n",
        "def pad_x(stadnamn_list):\n",
        "  X=[]\n",
        "  Y=[]\n",
        "  for stadnamn in stadnamn_list:\n",
        "    x,y = xysetfromstadnamn(stadnamn,tokens,start_stop_chars)\n",
        "    X+=x\n",
        "    Y+=y\n",
        "  X=pad_sequences(X,maxlen)\n",
        "  return X,np.asarray(Y)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JCdEKXFBw7K"
      },
      "source": [
        "# Preprocessing entire dataset\n",
        "# n_placenames=1400000\n",
        "#X,Y=pad_x(data.split('\\n')[:n_placenames])\n",
        "X,Y=pad_x(data.split('\\n'))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgg1D5JVTimL",
        "outputId": "0992f3a9-0047-41c9-cf04-d4beb53f4e04"
      },
      "source": [
        "print(X.shape,Y.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14522431, 53) (14522431,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzZb5q-9G9GD"
      },
      "source": [
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=7,train_size=0.75)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSKRo4o3UVZ7",
        "outputId": "cce1a966-8b90-468f-f2b8-c729989086da"
      },
      "source": [
        "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10891823, 53) (3630608, 53) (10891823,) (3630608,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn_7AtrYFc4A"
      },
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, n_tokens+1, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(200, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(n_tokens+1, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN172AroGSjD"
      },
      "source": [
        "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lIG8Z6ME8Tl",
        "outputId": "5c2c4436-7a57-461e-9de8-12b60353bd89"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 53)]              0         \n",
            "_________________________________________________________________\n",
            "token_and_position_embedding (None, 53, 32)            5632      \n",
            "_________________________________________________________________\n",
            "transformer_block (Transform (None, 53, 32)            10656     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 200)               6600      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 123)               24723     \n",
            "=================================================================\n",
            "Total params: 47,611\n",
            "Trainable params: 47,611\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYP03l7sHuuk",
        "outputId": "87f601c1-7862-4236-b292-2657189e3190"
      },
      "source": [
        "history = model.fit(X_train, Y_train, batch_size=32, epochs=2, validation_data=(X_test, Y_test))\n",
        "tfmodelfn='/content/drive/MyDrive/stadnamn/model2.tf'\n",
        "model.save(tfmodelfn)\n",
        "tokenfn=tfmodelfn+\"/assets/tokens.json\"\n",
        "with open(tokenfn,'w') as fh:\n",
        "  json.dump(tokens,fh)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "340370/340370 [==============================] - 2217s 7ms/step - loss: 2.0219 - accuracy: 0.4060 - val_loss: 1.8015 - val_accuracy: 0.4699\n",
            "Epoch 2/2\n",
            "340370/340370 [==============================] - 2203s 6ms/step - loss: 1.8607 - accuracy: 0.4524 - val_loss: 1.7910 - val_accuracy: 0.4731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses while saving (showing 5 of 65). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses while saving (showing 5 of 65). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/stadnamn/model2.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/stadnamn/model2.tf/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tgb9O4Y-dd9",
        "outputId": "d44068eb-99b2-4a36-ef0c-0af0a57edb8f"
      },
      "source": [
        "history"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "history\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTKuvYaMASgZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H10eOkUNArEV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckaRANF9BgvX"
      },
      "source": [
        "model=keras.models.load_model(tfmodelfn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnyXYqlwGb2n"
      },
      "source": [
        "tokenchar=dict([(v,k) for k,v in tokens.items()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUyzwOLZQbRY"
      },
      "source": [
        "# Storing tokens in model folder for convenience\n",
        "tokenfn=tfmodelfn+\"/assets/tokens.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WyC1VcNHe2k"
      },
      "source": [
        "with open(tokenfn,'w') as fh:\n",
        "  json.dump(tokens,fh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hwga6a2iw06"
      },
      "source": [
        "with open(tokenfn,'r') as fh:\n",
        "  tokens=json.load(fh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI21V5nLPyxb"
      },
      "source": [
        "def X_from_str(txt,tokens,start_stop_chars,maxlen):\n",
        "  A=[]\n",
        "  for t in txt:\n",
        "    X=[tokens[start_stop_chars[0]]]\n",
        "    for c in t:\n",
        "      X.append(tokens[c])\n",
        "    A.append(X)\n",
        "  return pad_sequences(A,maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJHyoPN2E_Sr"
      },
      "source": [
        "def str_from_X(X,tokenchar):\n",
        "  wordparts=[]\n",
        "  for a in X:\n",
        "    wordparts.append(''.join([tokenchar[i] for i in a[a>0]][1:]))\n",
        "  return wordparts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k2hMUNTHGgr"
      },
      "source": [
        "str_from_X(X_test[:10],tokenchar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGXcUVfYpGfP"
      },
      "source": [
        "def sampler(x,method='standard',temperature=1.0):\n",
        "  if method=='standard':\n",
        "    thetoken=np.argmax(x)\n",
        "  if method == 'temperature':\n",
        "    x = np.log(x) / temperature\n",
        "    exp_x = np.exp(x)\n",
        "    x = exp_x / np.sum(exp_x)\n",
        "    thetoken=np.argmax(np.random.multinomial(1,x*0.99999,1))\n",
        "  return thetoken"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FfmXkcZXa0U"
      },
      "source": [
        "def autocomplete(txt,tokens,tokenchar,start_stop_chars,maxlen,samplingmethod='standard',debug=True):\n",
        "  nextchar=''\n",
        "  sn=txt[0]\n",
        "  while nextchar != start_stop_chars[1]:\n",
        "    output=model.predict(X_from_str([sn],tokens,start_stop_chars,maxlen))[0]\n",
        "    nextchar=tokenchar[sampler(output,method=samplingmethod)]\n",
        "    sn+=nextchar\n",
        "    if debug:\n",
        "      print(sn)\n",
        "  sn=sn[:-1]\n",
        "  return sn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "665Y2IFGXngr"
      },
      "source": [
        "autocomplete(['Liv'],tokens,tokenchar,start_stop_chars,maxlen,samplingmethod='standard',debug=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix6TsjTnk61t"
      },
      "source": [
        "X_from_str(['Bu'],tokens,start_stop_chars,maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biNsMNoVJd31",
        "outputId": "c170b9f7-163a-4916-b303-3fcc7e655e82"
      },
      "source": [
        "from google.colab import files,drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-d8keKelEzt",
        "outputId": "757f25d8-8245-482c-fab6-02ceee77d388"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "import json\n",
        "np.seterr(divide = 'ignore')\n",
        "maxlen=53\n",
        "start_stop_chars=['@','$']\n",
        "\n",
        "tfmodelfn='/content/drive/MyDrive/stadnamn/model1.tf'\n",
        "model=keras.models.load_model(tfmodelfn)\n",
        "\n",
        "tokenfn=tfmodelfn+'/assets/tokens.json'\n",
        "with open(tokenfn,'r') as fh:\n",
        "  tokens=json.load(fh)\n",
        "tokenchar=dict([(v,k) for k,v in tokens.items()])\n",
        "\n",
        "def X_from_str(txt,tokens,start_stop_chars,maxlen):\n",
        "  A=[]\n",
        "  for t in txt:\n",
        "    X=[tokens[start_stop_chars[0]]]\n",
        "    for c in t:\n",
        "      X.append(tokens[c])\n",
        "    A.append(X)\n",
        "  return pad_sequences(A,maxlen)\n",
        "\n",
        "def str_from_X(X,tokenchar):\n",
        "  wordparts=[]\n",
        "  for a in X:\n",
        "    wordparts.append(''.join([tokenchar[i] for i in a[a>0]][1:]))\n",
        "  return wordparts\n",
        "\n",
        "def sampler(x,method='standard',temperature=1.0):\n",
        "  if method=='standard':\n",
        "    thetoken=np.argmax(x)\n",
        "  if method == 'temperature':\n",
        "    x = np.log(x) / temperature\n",
        "    exp_x = np.exp(x)\n",
        "    x = exp_x / np.sum(exp_x)\n",
        "    thetoken=np.argmax(np.random.multinomial(1,x*0.99999,1))\n",
        "  return thetoken\n",
        "\n",
        "def autocomplete(txt,tokens,tokenchar,start_stop_chars,maxlen,samplingmethod='standard',temperature=1.0,debug=True):\n",
        "  placenames=[]\n",
        "  for sn in txt:\n",
        "    nextchar=''\n",
        "    while nextchar != start_stop_chars[1]:\n",
        "      output=model.predict(X_from_str([sn],tokens,start_stop_chars,maxlen))[0]\n",
        "      nextchar=tokenchar[sampler(output,method=samplingmethod,temperature=temperature)]\n",
        "      sn+=nextchar\n",
        "      if debug:\n",
        "        print(sn)\n",
        "    sn=sn[:-1]\n",
        "    placenames.append(sn)\n",
        "  return placenames\n",
        "\n",
        "autocomplete(['Liv'],tokens,tokenchar,start_stop_chars,maxlen,samplingmethod='temperature',temperature=1.0,debug=False)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Liven']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiduB6bNcED-",
        "outputId": "44c8c70a-a0d4-411b-cc4a-febc18c95a41"
      },
      "source": [
        "autocomplete(['Liv','Nordr','Søndre','',''],tokens,tokenchar,start_stop_chars,maxlen,samplingmethod='temperature',temperature=1.0,debug=False)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Livåtjern',\n",
              " 'Nordre Golledalen',\n",
              " 'Søndre Palskløkklet',\n",
              " 'Vasshellet',\n",
              " 'Mollageren']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOiV_EHskmE7"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}